export const MODELS = [
  {
    id: 'google/gemini-2.0-flash-lite-001',
    hugging_face_id: '',
    name: 'Google: Gemini 2.0 Flash Lite',
    created: 1740506212,
    description:
      'Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.',
    context_length: 1048576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image', 'file'],
      output_modalities: ['text'],
      tokenizer: 'Gemini',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'fast'],
  },
  {
    id: 'google/gemini-2.0-flash-001',
    hugging_face_id: '',
    name: 'Google: Gemini 2.0 Flash',
    created: 1738769413,
    description:
      'Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.',
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image', 'file'],
      output_modalities: ['text'],
      tokenizer: 'Gemini',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'search'],
  },
  {
    id: 'google/gemini-2.5-flash-preview-05-20',
    hugging_face_id: '',
    name: 'Google: Gemini 2.5 Flash',
    created: 1747761924,
    description:
      'Gemini 2.5 Flash May 20th Checkpoint is Google\'s state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in "thinking" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nNote: This model is available in two variants: thinking and non-thinking. The output pricing varies significantly depending on whether the thinking capability is active. If you select the standard variant (without the ":thinking" suffix), the model will explicitly avoid generating thinking tokens. \n\nTo utilize the thinking capability and receive thinking tokens, you must choose the ":thinking" variant, which will then incur the higher thinking-output pricing. \n\nAdditionally, Gemini 2.5 Flash is configurable through the "max tokens for reasoning" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).',
    context_length: 1048576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'Gemini',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'search'],
  },
  {
    id: 'google/gemini-2.5-flash-preview:thinking',
    hugging_face_id: '',
    name: 'Google: Gemini 2.5 Flash (Thinking)',
    created: 1744914667,
    description:
      'Gemini 2.5 Flash is Google\'s state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in "thinking" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nNote: This model is available in two variants: thinking and non-thinking. The output pricing varies significantly depending on whether the thinking capability is active. If you select the standard variant (without the ":thinking" suffix), the model will explicitly avoid generating thinking tokens. \n\nTo utilize the thinking capability and receive thinking tokens, you must choose the ":thinking" variant, which will then incur the higher thinking-output pricing. \n\nAdditionally, Gemini 2.5 Flash is configurable through the "max tokens for reasoning" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).',
    context_length: 1048576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'Gemini',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'search', 'effort-control'],
  },
  {
    id: 'google/gemini-2.5-pro-preview',
    hugging_face_id: '',
    name: 'Google: Gemini 2.5 Pro',
    created: 1749137257,
    description:
      'Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n',
    context_length: 1048576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['file', 'image', 'text'],
      output_modalities: ['text'],
      tokenizer: 'Gemini',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'effort-control'],
  },
  {
    id: 'openai/gpt-image-1',
    hugging_face_id: '',
    name: 'OpenAI: GPT ImageGen',
    created: 1744824212,
    description:
      'OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to high. \n\nOpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'Other',
      instruct_type: null,
    },
    supported_parameters: ['image-generation'],
  },
  {
    id: 'openai/gpt-4o-mini',
    hugging_face_id: null,
    name: 'OpenAI: GPT-4o-mini',
    created: 1721260800,
    description:
      "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
    context_length: 128000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image', 'file'],
      output_modalities: ['text'],
      tokenizer: 'GPT',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'openai/gpt-4o',
    hugging_face_id: null,
    name: 'OpenAI: GPT-4o',
    created: 1715558400,
    description:
      'GPT-4o ("o" for "omni") is OpenAI\'s latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called ["im-also-a-good-gpt2-chatbot"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal',
    context_length: 128000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image', 'file'],
      output_modalities: ['text'],
      tokenizer: 'GPT',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'openai/gpt-4.1',
    hugging_face_id: '',
    name: 'OpenAI: GPT-4.1',
    created: 1744651385,
    description:
      'GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.',
    context_length: 1047576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'GPT',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'openai/gpt-4.1-mini',
    hugging_face_id: '',
    name: 'OpenAI: GPT-4.1 Mini',
    created: 1744651381,
    description:
      'GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.',
    context_length: 1047576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'GPT',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'openai/gpt-4.1-nano',
    hugging_face_id: '',
    name: 'OpenAI: GPT-4.1 Nano',
    created: 1744651369,
    description:
      'For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding – even higher than GPT‑4o mini. It’s ideal for tasks like classification or autocompletion.',
    context_length: 1047576,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'GPT',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'openai/o3-mini',
    hugging_face_id: '',
    name: 'OpenAI: o3 Mini',
    created: 1738351721,
    description:
      'OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.\n\nThis model supports the `reasoning_effort` parameter, which can be set to "high", "medium", or "low" to control the thinking time of the model. The default is "medium". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to "high".\n\nThe model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.',
    context_length: 200000,
    architecture: {
      modality: 'text->text',
      input_modalities: ['text'],
      output_modalities: ['text'],
      tokenizer: 'Other',
      instruct_type: null,
    },
    supported_parameters: ['effort-control'],
  },
  {
    id: 'openai/o4-mini',
    hugging_face_id: '',
    name: 'OpenAI: o4 Mini',
    created: 1744820942,
    description:
      'OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text'],
      output_modalities: ['text'],
      tokenizer: 'Other',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'reasoning', 'effort-control'],
  },
  {
    id: 'openai/o3',
    hugging_face_id: '',
    name: 'OpenAI: o3',
    created: 1744823457,
    description:
      'o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. Note that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text', 'file'],
      output_modalities: ['text'],
      tokenizer: 'Other',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'reasoning', 'effort-control'],
  },
  {
    id: 'anthropic/claude-3.5-haiku',
    hugging_face_id: null,
    name: 'Anthropic: Claude 3.5 Haiku',
    created: 1730678400,
    description:
      'Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image'],
      output_modalities: ['text'],
      tokenizer: 'Claude',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf'],
  },
  {
    id: 'anthropic/claude-3.7-sonnet',
    hugging_face_id: '',
    name: 'Anthropic: Claude 3.7 Sonnet',
    created: 1740422110,
    description:
      'Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image'],
      output_modalities: ['text'],
      tokenizer: 'Claude',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf'],
  },
  {
    id: 'anthropic/claude-3.7-sonnet:thinking',
    hugging_face_id: '',
    name: 'Anthropic: Claude 3.7 Sonnet (thinking)',
    created: 1740422110,
    description:
      'Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image'],
      output_modalities: ['text'],
      tokenizer: 'Claude',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'reasoning', 'effort-control'],
  },
  {
    id: 'anthropic/claude-sonnet-4',
    hugging_face_id: '',
    name: 'Anthropic: Claude Sonnet 4',
    created: 1747930371,
    description:
      'Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text'],
      output_modalities: ['text'],
      tokenizer: 'Claude',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf'],
  },
  {
    id: 'anthropic/claude-opus-4',
    hugging_face_id: '',
    name: 'Anthropic: Claude Opus 4',
    created: 1747931245,
    description:
      'Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['image', 'text'],
      output_modalities: ['text'],
      tokenizer: 'Claude',
      instruct_type: null,
    },
    supported_parameters: ['vision', 'pdf', 'reasoning'],
  },
  {
    id: 'meta-llama/llama-3.3-70b-instruct:free',
    hugging_face_id: 'meta-llama/Llama-3.3-70B-Instruct',
    name: 'Meta: Llama 3.3 70B',
    created: 1733506137,
    description:
      'The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)',
    context_length: 131072,
    architecture: {
      modality: 'text->text',
      input_modalities: ['text'],
      output_modalities: ['text'],
      tokenizer: 'Llama3',
      instruct_type: 'llama3',
    },
    supported_parameters: ['fast'],
  },
  {
    id: 'meta-llama/llama-4-scout:free',
    hugging_face_id: 'meta-llama/Llama-4-Scout-17B-16E-Instruct',
    name: 'Meta: Llama 4 Scout',
    created: 1743881519,
    description:
      'Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.',
    context_length: 200000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image'],
      output_modalities: ['text'],
      tokenizer: 'Llama4',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'meta-llama/llama-4-maverick:free',
    hugging_face_id: 'meta-llama/Llama-4-Maverick-17B-128E-Instruct',
    name: 'Meta: Llama 4 Maverick',
    created: 1743881822,
    description:
      'Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.',
    context_length: 128000,
    architecture: {
      modality: 'text+image->text',
      input_modalities: ['text', 'image'],
      output_modalities: ['text'],
      tokenizer: 'Llama4',
      instruct_type: null,
    },
    supported_parameters: ['vision'],
  },
  {
    id: 'deepseek/deepseek-r1-0528:free',
    hugging_face_id: 'deepseek-ai/DeepSeek-R1-0528',
    name: 'DeepSeek: R1 (0528)',
    created: 1748455170,
    description:
      "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
    context_length: 163840,
    architecture: {
      modality: 'text->text',
      input_modalities: ['text'],
      output_modalities: ['text'],
      tokenizer: 'DeepSeek',
      instruct_type: 'deepseek-r1',
    },
    supported_parameters: ['reasoning'],
  },
  {
    id: 'x-ai/grok-3-mini-beta',
    hugging_face_id: '',
    name: 'xAI: Grok 3 Mini',
    created: 1744240195,
    description:
      'Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answers immediately, Grok 3 Mini thinks before responding. It’s ideal for reasoning-heavy tasks that don’t demand extensive domain knowledge, and shines in math-specific and quantitative use cases, such as solving challenging puzzles or math problems.\n\nTransparent "thinking" traces accessible. Defaults to low reasoning, can boost with setting `reasoning: { effort: "high" }`\n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n',
    context_length: 131072,
    architecture: {
      modality: 'text->text',
      input_modalities: ['text'],
      output_modalities: ['text'],
      tokenizer: 'Grok',
      instruct_type: null,
    },
    supported_parameters: ['reasoning', 'effort-control'],
  },
  {
    id: 'x-ai/grok-3-beta',
    hugging_face_id: '',
    name: 'xAI: Grok 3',
    created: 1744240068,
    description:
      "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\nExcels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. \n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
    context_length: 131072,
    architecture: {
      modality: 'text->text',
      input_modalities: ['text'],
      output_modalities: ['text'],
      tokenizer: 'Grok',
      instruct_type: null,
    },
    supported_parameters: [],
  },
  {
    id: 'eva-unit-01/eva-qwen-2.5-32b',
    hugging_face_id: 'EVA-UNIT-01/EVA-Qwen2.5-32B-v0.2',
    name: 'EVA Qwen2.5 32B',
    created: 1731104847,
    description:
      'EVA Qwen2.5 32B is a roleplaying/storywriting specialist model. It\'s a full-parameter finetune of Qwen2.5-32B on mixture of synthetic and natural data.\n\nIt uses Celeste 70B 0.1 data mixture, greatly expanding it to improve versatility, creativity and "flavor" of the resulting model.',
    context_length: 16384,
    architecture: {
      modality: 'text->text',
      input_modalities: ['text'],
      output_modalities: ['text'],
      tokenizer: 'Qwen',
      instruct_type: 'chatml',
    },
    supported_parameters: ['fast', 'vision'],
  },
];
